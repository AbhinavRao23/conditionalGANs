{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI4gn4cmaHpY"
      },
      "source": [
        "# ECE 50024 Final Project - Conditional Generative Adversarial Networks by Abhinav Rao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGahGKc_aHpa"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42KewGZ4npi0",
        "outputId": "e5b1e3e5-dab8-4816-ac89-3855b4d2d752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Standard Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import os\n",
        "import pickle \n",
        "%matplotlib inline\n",
        "\n",
        "# Neural Network Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "#for colab purposes only\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRUPTXWnaHpb"
      },
      "source": [
        "<b>Accessing GPU if available and defining Log Folder path</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BovbyjRhnq1C"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logPath='/content/gdrive/MyDrive/Colab Notebooks/GAN/log11'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amQAg8fkaHpc"
      },
      "source": [
        "<b>Selecting Hyperparameters and saving to log file</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qi4cFRznyJs"
      },
      "outputs": [],
      "source": [
        "batchSize = 32 #inputs in one pass of gradient\n",
        "leakyReLUNegSlope = 0.2 #slope for negative part of leaky ReLU, 0 means ReLU\n",
        "dropoutRate = 0.3 #what factor of neurons to turn off\n",
        "dataDim = 784 # 28x28\n",
        "labelDim = 10 # 0-9 \n",
        "noiseDim = 100 # Z dimension \n",
        "learningRateG = 1e-4 # learning rate (alpha) of generator\n",
        "learningRateD = 1e-4 # learning rate (alpha) of discriminator\n",
        "nEpochs = 100 #how many training cycles/epochs\n",
        "nCritic = 5 #wasserstein number of critical steps - ncritic = 1 --> vanilla GAN\n",
        "nLog = 10\n",
        "\n",
        "hyperParamDict = {\n",
        "    'batch size':batchSize,\n",
        "    'generator learning rate': learningRateG,\n",
        "    'discriminator learning rate': learningRateD,\n",
        "    'number of epochs':nEpochs,\n",
        "    'dropout rate': dropoutRate,\n",
        "    'leaky RELU negative slope': leakyReLUNegSlope,\n",
        "    'wasserstein nCritic': nCritic}\n",
        "  \n",
        "\n",
        "with open(os.path.join(logPath,'hyperparameters.txt'), 'w') as logfile:\n",
        "    logfile.write(json.dumps(hyperParamDict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS10d2q7aHpc"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrGFL0JBaHpd"
      },
      "source": [
        "<b>Downloading data and Pre-Processing</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsGrx7Z9n08k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b1c0ad7-2e91-4a9b-c41f-482d8fbf57d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 156284977.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 54639464.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 55597110.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21309316.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(), #converts to pytorch object called tensor, like array for numpy\n",
        "    torchvision.transforms.Normalize([0.5], [0.5]) #Normalize to mean 0.5, stdev = 0.5\n",
        "])\n",
        "\n",
        "\n",
        "dataLoader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('data', #make new folder and save in it\n",
        "                               train=True, \n",
        "                               download=True, \n",
        "                               transform=transform),\n",
        "    batch_size=batchSize, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDzVRFd9aHpd"
      },
      "source": [
        "<b>Defining Discriminator</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NCrAvnSn20v"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() #Inherits the init of superclass \n",
        "        self.labelEmbedding = nn.Embedding(labelDim, labelDim) #labelsize, onehot\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(dataDim + labelDim, 1000),\n",
        "            nn.LeakyReLU(leakyReLUNegSlope, inplace=True),\n",
        "            nn.Dropout(dropoutRate),\n",
        "            nn.Linear(1000, 500),\n",
        "            nn.LeakyReLU(leakyReLUNegSlope, inplace=True),\n",
        "            nn.Dropout(dropoutRate),\n",
        "            nn.Linear(500, 250),\n",
        "            nn.LeakyReLU(leakyReLUNegSlope, inplace=True),\n",
        "            nn.Dropout(dropoutRate),\n",
        "            nn.Linear(250, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        x = x.view(x.size(0), dataDim)\n",
        "        c = self.labelEmbedding(y)\n",
        "        x = torch.cat([x, c], 1)\n",
        "        out = self.model(x)\n",
        "        return out.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX5BTpCmaHpd"
      },
      "source": [
        "<b>Defining Generator</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a_yDAoyn5PP"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.labelEmbedding = nn.Embedding(labelDim, labelDim)\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noiseDim + labelDim , 250),\n",
        "            nn.LeakyReLU(leakyReLUNegSlope, inplace=True),\n",
        "            nn.Linear(250, 500),\n",
        "            nn.LeakyReLU(leakyReLUNegSlope, inplace=True),\n",
        "            nn.Linear(500, 1000),\n",
        "            nn.LeakyReLU(leakyReLUNegSlope, inplace=True),\n",
        "            nn.Linear(1000, dataDim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, z, labels):\n",
        "        z = z.view(z.size(0), noiseDim)\n",
        "        c = self.labelEmbedding(labels)\n",
        "        x = torch.cat([z, c], 1)\n",
        "        out = self.model(x)\n",
        "        return out.view(x.size(0),int(np.sqrt(dataDim)),int(np.sqrt(dataDim)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yIdpAf7aHpe"
      },
      "source": [
        "### Defining training functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3xGqlCAaHpe"
      },
      "source": [
        "<b>Instantiating Generator and Discriminator and defining Loss and Optimizers</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMflz2cdn7Vq"
      },
      "outputs": [],
      "source": [
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "loss = nn.BCELoss()\n",
        "discriminatorOptimizer = torch.optim.Adam(discriminator.parameters(), lr=learningRateD)\n",
        "generatorOptimizer = torch.optim.Adam(generator.parameters(), lr=learningRateG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vWaMeA9aHpe"
      },
      "source": [
        "<b>Instantiating Generator and Discriminator and defining Loss and Optimizers</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxMG3b7kn_Jh"
      },
      "outputs": [],
      "source": [
        "def trainGenerator(batchSize, discriminator, generator, generatorOptimizer, loss):\n",
        "    generatorOptimizer.zero_grad()\n",
        "    z = Variable(torch.randn(batchSize, noiseDim)).to(device)\n",
        "    generatedLabels = Variable(torch.LongTensor(np.random.randint(0, labelDim, batchSize))).to(device)\n",
        "    generatedData = generator(z, generatedLabels)\n",
        "    generatorLoss = loss(discriminator(generatedData, generatedLabels), Variable(torch.ones(batchSize)).to(device))\n",
        "    generatorLoss.backward()\n",
        "    generatorOptimizer.step()\n",
        "    return generatorLoss.item()\n",
        "\n",
        "def trainDiscriminator(batchSize, discriminator, generator, discriminatorOptimizer, loss, realData, realLabels):\n",
        "    \n",
        "    discriminatorOptimizer.zero_grad()\n",
        "    \n",
        "    realLoss = loss(discriminator(realData, realLabels), Variable(torch.ones(batchSize)).to(device))\n",
        "    \n",
        "    z = Variable(torch.randn(batchSize, noiseDim)).to(device)\n",
        "    generatedLabels = Variable(torch.LongTensor(np.random.randint(0, labelDim, batchSize))).to(device)\n",
        "    generatedData = generator(z, generatedLabels)\n",
        "    generatorLoss = loss(discriminator(generatedData, generatedLabels), Variable(torch.zeros(batchSize)).to(device))\n",
        "    \n",
        "    discriminatorLoss = realLoss + generatorLoss\n",
        "    discriminatorLoss.backward()\n",
        "    discriminatorOptimizer.step()\n",
        "    return discriminatorLoss.item()\n",
        "\n",
        "def trainGAN(dataLoader, generator, discriminator, nEpochs=50, nLog = 10, nCritic=1):\n",
        "    times = []\n",
        "    discriminatorLosses = []\n",
        "    generatorLosses = []\n",
        "    log = {}\n",
        "\n",
        "    for epoch in range(nEpochs):\n",
        "        print('Epoch '+str(epoch+1), end=' - Discriminator Loss: ')\n",
        "        start = time.time()\n",
        "        for i, (data, labels) in enumerate(dataLoader):\n",
        "            realData = Variable(data).to(device)\n",
        "            realLabels = Variable(labels).to(device)\n",
        "            generator.train()\n",
        "\n",
        "            for i in range(nCritic):\n",
        "                discriminatorLoss = trainDiscriminator(len(realData), discriminator,\n",
        "                                                generator, discriminatorOptimizer, loss,\n",
        "                                                realData, realLabels)\n",
        "\n",
        "\n",
        "            generatorLoss = trainGenerator(batchSize, discriminator, generator, generatorOptimizer, loss)\n",
        "        stop = time.time()\n",
        "        print(round(discriminatorLoss,4), end=' - Generator Loss: ')\n",
        "        discriminatorLosses.append(discriminatorLoss)\n",
        "        print(round(generatorLoss,4))\n",
        "        generatorLosses.append(generatorLoss)\n",
        "        times.append(stop - start)\n",
        "\n",
        "        if (epoch+1)%(nEpochs//nLog)==0:\n",
        "            log[str(epoch+1)] = generateSamples(generator, 10, random = False)\n",
        "\n",
        "    performanceDF = pd.DataFrame(columns = ['epoch','time','discriminatorLoss','generatorLoss'])\n",
        "    performanceDF['epoch'] = range(nEpochs)\n",
        "    performanceDF['time'] = times\n",
        "    performanceDF['discriminatorLoss'] = discriminatorLosses\n",
        "    performanceDF['generatorLoss'] = generatorLosses\n",
        "\n",
        "    return generator, performanceDF, log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KGovpJZoA3l"
      },
      "source": [
        "<b>Diagnosis Functions</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9x6eSh65nzl"
      },
      "outputs": [],
      "source": [
        "def generateSamples(generator, nSamples, random=True):\n",
        "    if not random:\n",
        "        return generator(torch.randn(nSamples, noiseDim).to(device),\n",
        "                     torch.LongTensor(np.arange(nSamples)%10).to(device))\n",
        "    return generator(torch.randn(nSamples, noiseDim).to(device),\n",
        "                     torch.LongTensor(np.random.randint(0, labelDim, nSamples)).to(device))\n",
        "\n",
        "def displaySamples(samples, title = 'Digits', savePath = False):\n",
        "    nSamples = samples.shape[0]\n",
        "    nrows = math.ceil(nSamples/10)\n",
        "    fig,ax = plt.subplots(nrows = nrows, ncols = 10, figsize = (15,2*nrows),squeeze=False)\n",
        "    for i in range(nrows*10):\n",
        "        if i<nSamples:\n",
        "            pixels = samples[i].cpu().detach().numpy()\n",
        "            ax[i//10][i%10].imshow(pixels, cmap='gray')\n",
        "            ax[i//10][i%10].set_yticklabels([])\n",
        "            ax[i//10][i%10].set_xticklabels([])\n",
        "        else:\n",
        "            fig.delaxes(ax[i//10][i%10])\n",
        "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "    plt.suptitle(title)\n",
        "    if savePath:\n",
        "        plt.savefig(os.path.join(savePath,title))\n",
        "    plt.show()\n",
        "\n",
        "def saveLog(trainedGenerator, trainingPerformance, dataLog, logPath):\n",
        "    trainingPerformance.to_csv(os.path.join(logPath, 'trainingLog.csv'),index=False)\n",
        "    for key,value in dataLog.items():\n",
        "        displaySamples(value,title = 'Epoch '+key+ ': All Digits', savePath = logPath)\n",
        "    modelfile = open(os.path.join(logPath,'generator.pkl'), 'wb')\n",
        "    pickle.dump(trainedGenerator, modelfile)\n",
        "    modelfile.close()\n",
        "    print('Training details saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF9GGsQvaHpf"
      },
      "source": [
        "<b></b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFf1gJJAoCh-",
        "outputId": "748be691-7970-4e8b-e3c8-d03c540b826a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Discriminator Loss: 0.0015 - Generator Loss: 10.8443\n",
            "Epoch 2 - Discriminator Loss: 0.0084 - Generator Loss: 5.933\n",
            "Epoch 3 - Discriminator Loss: 0.1066 - Generator Loss: 9.1138\n",
            "Epoch 4 - Discriminator Loss: 0.2829 - Generator Loss: 4.3386\n",
            "Epoch 5 - Discriminator Loss: 0.1418 - Generator Loss: 5.7566\n",
            "Epoch 6 - Discriminator Loss: 0.3385 - Generator Loss: 2.9577\n",
            "Epoch 7 - Discriminator Loss: 0.5205 - Generator Loss: 3.0595\n",
            "Epoch 8 - Discriminator Loss: 0.4873 - Generator Loss: 3.4417\n",
            "Epoch 9 - Discriminator Loss: 0.6798 - Generator Loss: 2.7812\n",
            "Epoch 10 - Discriminator Loss: 0.4842 - Generator Loss: 2.7815\n",
            "Epoch 11 - Discriminator Loss: 0.7647 - Generator Loss: 1.8137\n",
            "Epoch 12 - Discriminator Loss: 0.601 - Generator Loss: 2.3609\n",
            "Epoch 13 - Discriminator Loss: 0.8558 - Generator Loss: 1.5876\n",
            "Epoch 14 - Discriminator Loss: 0.851 - Generator Loss: 1.7583\n",
            "Epoch 15 - Discriminator Loss: 0.7507 - Generator Loss: 1.8943\n",
            "Epoch 16 - Discriminator Loss: 0.9273 - Generator Loss: 1.9775\n",
            "Epoch 17 - Discriminator Loss: 0.8081 - Generator Loss: 2.0136\n",
            "Epoch 18 - Discriminator Loss: 0.854 - Generator Loss: 1.9446\n",
            "Epoch 19 - Discriminator Loss: 0.9962 - Generator Loss: 1.4915\n",
            "Epoch 20 - Discriminator Loss: 0.8409 - Generator Loss: 1.6845\n",
            "Epoch 21 - Discriminator Loss: 1.1079 - Generator Loss: 1.6242\n",
            "Epoch 22 - Discriminator Loss: 0.7924 - Generator Loss: 1.5304\n",
            "Epoch 23 - Discriminator Loss: 0.9398 - Generator Loss: 1.2739\n",
            "Epoch 24 - Discriminator Loss: 0.7394 - Generator Loss: 0.9698\n",
            "Epoch 25 - Discriminator Loss: 0.8321 - Generator Loss: 1.6221\n",
            "Epoch 26 - Discriminator Loss: 1.2735 - Generator Loss: 1.2734\n",
            "Epoch 27 - Discriminator Loss: 0.8975 - Generator Loss: 1.1951\n",
            "Epoch 28 - Discriminator Loss: 1.0708 - Generator Loss: 1.2382\n",
            "Epoch 29 - Discriminator Loss: 1.0435 - Generator Loss: 1.2118\n",
            "Epoch 30 - Discriminator Loss: 0.9485 - Generator Loss: 1.443\n",
            "Epoch 31 - Discriminator Loss: 1.0757 - Generator Loss: 1.2326\n",
            "Epoch 32 - Discriminator Loss: 0.9406 - Generator Loss: 1.3269\n",
            "Epoch 33 - Discriminator Loss: 0.8725 - Generator Loss: 1.7911\n",
            "Epoch 34 - Discriminator Loss: 1.1951 - Generator Loss: 1.2365\n",
            "Epoch 35 - Discriminator Loss: 1.0291 - Generator Loss: 1.4303\n",
            "Epoch 36 - Discriminator Loss: 1.1218 - Generator Loss: 1.2371\n",
            "Epoch 37 - Discriminator Loss: 1.244 - Generator Loss: 1.5965\n",
            "Epoch 38 - Discriminator Loss: 1.0723 - Generator Loss: 1.1417\n",
            "Epoch 39 - Discriminator Loss: 0.7659 - Generator Loss: 1.6134\n",
            "Epoch 40 - Discriminator Loss: 1.1348 - Generator Loss: 1.036\n",
            "Epoch 41 - Discriminator Loss: 0.9764 - Generator Loss: 1.4339\n",
            "Epoch 42 - Discriminator Loss: 0.8878 - Generator Loss: 1.4691\n",
            "Epoch 43 - Discriminator Loss: 1.0698 - Generator Loss: 1.4462\n",
            "Epoch 44 - Discriminator Loss: 1.0107 - Generator Loss: 1.1113\n",
            "Epoch 45 - Discriminator Loss: 1.1119 - Generator Loss: 1.294\n",
            "Epoch 46 - Discriminator Loss: 1.0484 - Generator Loss: 1.146\n",
            "Epoch 47 - Discriminator Loss: 1.1184 - Generator Loss: 1.2514\n",
            "Epoch 48 - Discriminator Loss: 0.9846 - Generator Loss: 1.0868\n",
            "Epoch 49 - Discriminator Loss: "
          ]
        }
      ],
      "source": [
        "trainedGenerator, trainingPerformance, dataLog = trainGAN(dataLoader, generator, discriminator, nEpochs= nEpochs, nLog = nLog, nCritic=nCritic)\n",
        "saveLog(trainedGenerator, trainingPerformance, dataLog, logPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a5dBjAvsybP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copVDPSTs1MJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}